det(D_sqrt)
# utilisons la fonction solve() de R pour calculer l'inverse d'une matrice
D_inv <- solve(D_sqrt)
D_inv
# Calcul de L
L <- D_inv %*%  A %*% D_inv
L
# test avec la précion d'ordre 5
round(L[1:10,1:10],precision)
### Etape 3: Calculons les vecteur propres et définissons la matrice X
#avec les k premiers vecteurs propres de L
#vecteurs propres
eig_vec <- eigen(L)$vectors
#eig_vec
dim(eig_vec)
# choix des nombres de classes (k)
#k <- 3
# calcul de 3 premiers vecteurs propres (k premiers valeurs propres)
X <- eig_vec[,(1 : k)]
X
### Etape 4: Calcul de la matrice Y en utilisant la matrice X:
#renormalisons chaque ligne de la matrice X pour avoir la longueur unité
# Creations d'une matrice vide avec n lignes et k colonnes
Y <- matrix(0, nrow = n, ncol = k)
# FIci pour chaque élément de X, nous divisons par la racine carré de la somme chaque élément de X au carré
for(i in 1:n){
for(j in 1:k){
Y[i,j] <- X[i,j] / sqrt(sum(X[i,])^2)
}
}
# Normalisons la matrice Y
#Y
## Etape 5: utilisons l'algorithme de K-Means
# traitons chaque ligne de Y comme point de R^k, régroupons les en k régroupements via K-means
km <- kmeans(Y, k, nstart = 2000, algorithm = c("MacQueen"))
return(list(Classe=km$cluster, Inerties=fdm2id::intern(as.numeric(km$cluster), Y, eval = c ("intraclass", "interclass"), type = c("global"))))
# inerties(Y, as.numeric(km$cluster))[-3]
}
#Veiller à choisir le sigma de plus petit I_intra/I_tot.
spec_clus(sp$x, 0.05, 2)
#Veiller à choisir le sigma de plus petit I_intra/I_tot.
Itot=spec_clus(sp$x, 0.05, 2)$Inerties$intraclass +spec_clus(sp$x, 0.05, 2)$Inerties$interclass
spec_clus(sp$x, 0.05, 2)$Inerties$intraclass
spec_clus(sp$x, 0.05, 2)
spec_clus(sp$x, 0.05, 2)$Inerties
spec_clus(sp$x, 0.05, 2)$Inerties[1]
View(data)
#Veiller à choisir le sigma de plus petit I_intra/I_tot.
Itot=spec_clus(sp$x, 0.05, 2)$Inerties[1] + spec_clus(sp$x, 0.05, 2)$Inerties[2]
rap_iner=spec_clus(sp$x, 0.05, 2)$Inerties[1]/Itot
rap_iner
spec_clus(sp$x, 0.05, 2)$Inerties[1]
spec_clus(sp$x, 0.05, 2)$Inerties[1]/Itot
rap_iner=spec_clus(sp$x, 0.05, 2)$Inerties[1]
rap_iner/Itot
(spec_clus(sp$x, 0.05, 2)$Inerties[1]/Itot)[1]
vector(NA, mode = integer, length = length(sigma))
vector( mode = integer, length = length(sigma))
vector(mode= "integer", length = length(sigma))
rap_mnist= vector(mode= "integer", length = length(sigma))
set.seed(2)
sp<-mlbench.spirals(300,1.5,0.05)
plot(sp$x, col= sp$classes)# on s'aperçoit que la plage des valeurs de x mesure environ 2 unités de x, Donc on choisit:
d=2; k=length(unique(sp$classes))
##Trouver le meilleur sigma
(sigma=c(0.05,0.085,0.09,seq(0.05*d, 0.35*d, 0.05)))
# implémenter sigma avec le rapport des inerties de Y
nn = matrix(NA, nrow = length(sigma), ncol = nrow(sp$x))
rap_iner= vector(mode= "integer", length = length(sigma))
for (j in 1:length(sigma) ){
#nn[j,]=spec_clus(sp$x, sigma[j], k)
rap_spi=[j]=spec_clus(sp$x, sigma[j], k)$Inerties[1]/(spec_clus(sp$x,sigma[j], k)$Inerties[1] + spec_clus(sp$x, sigma[j], k)$Inerties[2])
set.seed(2)
sp<-mlbench.spirals(300,1.5,0.05)
plot(sp$x, col= sp$classes)# on s'aperçoit que la plage des valeurs de x mesure environ 2 unités de x, Donc on choisit:
d=2; k=length(unique(sp$classes))
##Trouver le meilleur sigma
(sigma=c(0.05,0.085,0.09,seq(0.05*d, 0.35*d, 0.05)))
# implémenter sigma avec le rapport des inerties de Y
nn = matrix(NA, nrow = length(sigma), ncol = nrow(sp$x))
rap_iner= vector(mode= "integer", length = length(sigma))
for (j in 1:length(sigma) ){
#nn[j,]=spec_clus(sp$x, sigma[j], k)
rap_spi[j]=spec_clus(sp$x, sigma[j], k)$Inerties[1]/(spec_clus(sp$x,sigma[j], k)$Inerties[1] + spec_clus(sp$x, sigma[j], k)$Inerties[2])
}
##Trouver le meilleur sigma
(sigma=c(0.05,0.085,0.09,seq(0.05*d, 0.35*d, 0.05)))
# implémenter sigma avec le rapport des inerties de Y
nn = matrix(NA, nrow = length(sigma), ncol = nrow(sp$x))
rap_spi= vector(mode= "integer", length = length(sigma))
for (j in 1:length(sigma) ){
#nn[j,]=spec_clus(sp$x, sigma[j], k)
rap_spi[j]=spec_clus(sp$x, sigma[j], k)$Inerties[1]/(spec_clus(sp$x,sigma[j], k)$Inerties[1] + spec_clus(sp$x, sigma[j], k)$Inerties[2])
}
(sigma_opt_spi=sigma[which.min(rap_spi)])
#nn
set.seed(2)
sp<-mlbench.spirals(300,1.5,0.05)
plot(sp$x, col= sp$classes)# on s'aperçoit que la plage des valeurs de x mesure environ 2 unités de x, Donc on choisit:
d=2; k=length(unique(sp$classes))
##Trouver le meilleur sigma
(sigma=c(0.05,0.085,0.09,seq(0.05*d, 0.35*d, 0.05)))
# implémenter sigma avec le rapport des inerties de Y
nn = matrix(NA, nrow = length(sigma), ncol = nrow(sp$x))
rap_spi= vector(mode= "integer", length = length(sigma))
for (j in 1:length(sigma) ){
#nn[j,]=spec_clus(sp$x, sigma[j], k)
rap_spi[j]=spec_clus(sp$x, sigma[j], k)$Inerties[1]/(spec_clus(sp$x,sigma[j], k)$Inerties[1] +
spec_clus(sp$x, sigma[j], k)$Inerties[2])
}
(sigma_opt_spi=sigma[which.min(rap_spi)])
#nn
data = read_mnist()
#View(data)
images_train=data[["train"]][["images"]][1:1000,] # data$train$images[1:1000,]
save(images_train, file="mnist_reduit")
load("mnist_reduit")
labels_train= data$train$labels[1:1000]
save(labels_train, file="labels_reduit")
load("labels_reduit")
#par(mfrow=c(3,4))
for (k in 1:6)# 12_1ère images
m=matrix(images_train[k,], nrow =28, ncol =28 )
m=t(apply(m, 1, rev))# rev transpose l'odre des éléments d'1 vect.: la dernière col. se retrouve en 1ere position ds la mat.
image(m)
m
k=length(unique(labels_train))
sigma =seq(4,13)#spec_clus fct uniquement pour ces valeurs
mm = matrix(NA, nrow = length(sigma), ncol = nrow(images_train))
rap_mnist= vector(mode= "integer", length = length(sigma))
for (j in 1:length(sigma) ){
#mm[j,]=spec_clus(images_train, sigma[j], k)
rap_mnist[j]=spec_clus(images_train, sigma[j], k)$Inerties[1]/(spec_clus(images_train,sigma[j], k)$Inerties[1] +
spec_clus(images_train, sigma[j], k)$Inerties[2])
}
sigma_opt_mnist=sigma[which.min(rap_mnist)]
(sigma_opt_mnist=sigma[which.min(rap_mnist)])
library(npsurv); library(tidyverse); library(survival); library(survminer); library(ISwR)
data("leukemia")
leukemia
Time=leukemia$L
status=ifelse(leukemia$R==Inf,0, 1)
df0=cbind.data.frame(Time, status, group=leukemia$group)
base=Surv(df0$Time, df0$status)
Skm <- summary(survfit(base~1, data =df0))$surv
Skm.grouped.by.treatment=survfit(base~df0$group, data =df0)
ggsurvplot(Skm.grouped.by.treatment, data = df0,  #combine = TRUE, Not Combine curves
risk.table =FALSE,                  # Add risk table
conf.int = TRUE,                    # Add confidence interval
conf.int.style = "ribbon",            # CI style, use "step" or "ribbon"
censor = FALSE,                     # Remove censor points  # Clean risk table
palette = "jco")
survdiff(base~ df0$group)# par défaut rho =0 => test Mantel-Haenszel= log-Rank Non stratifié
survdiff(base~ df0$group, rho=1)#rho=1 => test log-Rank Peto-Wilcoxon=Variante Non stratifié du log-Rank
data("melanom")
head(melanom)
#La fonction Surv prend en arg. "event"=l'indicateur d'état, qui normalement vérifie ce code: 0=vivant, 1=mort.
#Donc dans 1 1er tps:
base=Surv(melanom$days, melanom$status==1)
surv.Global=survfit(base~1, data =melanom)
ggsurvplot(surv.Global, data = melanom,  #combine = TRUE, Not Combine curves
risk.table =FALSE,                  # Add risk table
conf.int = TRUE,                    # Add confidence interval
conf.int.style = "ribbon",            # CI style, use "step" or "ribbon"
censor = FALSE,                     # Remove censor points  # Clean risk table
palette = "jco")
#OU:
# Skm <- summary(surv.Global)$surv
# time.skm.survival=summary(surv.Global)$time
#plot(time.skm.survival, Skm, col='purple', main="Estimateur de la Survie globale par Kaplan Meier de (days, status€{0,1})", xlab="dates", ylab="Survie")
#object surv groupé par sexe:
surv.grouped.by.Sex=survfit(base~melanom$sex, data= melanom)
ggsurvplot(surv.grouped.by.Sex, data = melanom,  #combine = TRUE, Not Combine curves
risk.table =FALSE,                  # Add risk table
conf.int = TRUE,                    # Add confidence interval
conf.int.style = "ribbon",            # CI style, use "step" or "ribbon"
censor = FALSE,                     # Remove censor points  # Clean risk table
palette = "jco")
# OU:
# Skm <- summary(surv.grouped.by.Sex)$surv
# time.skm.survival=summary(surv.grouped.by.Sex)$time
#plot(time.skm.survival, Skm, col='red', main="Estimateur de la Survie selon le sexe par Kaplan Meier de (days, status€{0,1})", xlab="dates", ylab="Survie")
#object survie selon la var. ulc:
surv.grouped.by.Ulc=survfit(base~melanom$ulc, data= melanom)
ggsurvplot(surv.grouped.by.Ulc, data = melanom,  #combine = TRUE, Not Combine curves
risk.table =FALSE,                  # Add risk table
conf.int = TRUE,                    # Add confidence interval
conf.int.style = "ribbon",            # CI style, use "step" or "ribbon"
censor = FALSE,                     # Remove censor points  # Clean risk table
palette = "jco")
#base=Surv(melanom$days, melanom$status)
survdiff(base~ melanom$sex)#, rho=0 par défault
survdiff(base~ melanom$ulc)
survdiff(base~ melanom$sex + melanom$ulc )# ttes les combinaisons poss. entre modalités de "sex" et "ulc"
(surv.Sex.Strat.by.ulc=survdiff(base~ melanom$sex + strata(melanom$ulc) ))#,data= melanom par défaut il le trouve dans base
(surv.Ulc.Strat.by.Sex=survdiff(base~ melanom$ulc + strata(melanom$sex) ))
fitPbc <- coxph(Surv(time, status == 2) ~ age + factor(edema) + log(bili) + log(protime) + log(albumin), data = pbc)
(fitPbc <- coxph(Surv(time, status == 2) ~ age + factor(edema) + log(bili) + log(protime) + log(albumin), data = pbc))
(mydata <- expand.grid(age=c(50,60), edema = c(0.5,1), bili = mean(pbc$bili), protime=mean(na.omit(pbc$protime)), albumin = mean(pbc$albumin ) ))
survfit(fitPbc,newdata = mydata)
library(mlbench)
library(dslabs)
library(fdm2id)
library(flexclust)
spec_clus <- function (data, sigma, k){
precision <- 25 # définie la precision
my.data <- as.matrix(data) #créé une matrice de 150 lignes et 2 colonnes
n <- nrow(my.data) # n = nombre des colonnes du jeu des données
S <- my.data # charge les données dans une nouvelle variable
A <- matrix(rep(0,n*n) ,nrow = n ,ncol=n) # création  d'une matrice vide de n*n
## Algorithme de regroupement spectral
### Etape 1: Création d'une matrice (matrice d'affinité)__
for (i in 1:n){
for(j in 1:n){
if (i != j){
# distance euclidienne
A[i,j] <- exp(- sqrt(sum((S[i,]-S[j,])^2)) /(2*sigma^2))
}
}
}
# test avec la précion d'ordre 5
round(A[1:10,1:10],precision)
### Etape 2: Calcul de la matrice D et construction de la matrice L
#2.1 Calcul de la matrice D
D <- diag(n) # création d'une matrice diagonale vide
for (i in 1:n){
#  somme de chaque ligne de A et insertion dans la matrice diagonale D
D[i,i] <- sum(A[i,])
}
# test avec la précion d'ordre 5
round(D[1:10,1:10],precision)
### 2.2 Construction de la matrice L en utilisant D
# calcul de la racine carré de chaque élément de la matrice D
D_sqrt <- sqrt(D)
det(D_sqrt)
# utilisons la fonction solve() de R pour calculer l'inverse d'une matrice
D_inv <- solve(D_sqrt)
D_inv
# Calcul de L
L <- D_inv %*%  A %*% D_inv
L
# test avec la précion d'ordre 5
round(L[1:10,1:10],precision)
### Etape 3: Calculons les vecteur propres et définissons la matrice X
#avec les k premiers vecteurs propres de L
#vecteurs propres
eig_vec <- eigen(L)$vectors
#eig_vec
dim(eig_vec)
# choix des nombres de classes (k)
#k <- 3
# calcul de 3 premiers vecteurs propres (k premiers valeurs propres)
X <- eig_vec[,(1 : k)]
X
### Etape 4: Calcul de la matrice Y en utilisant la matrice X:
#renormalisons chaque ligne de la matrice X pour avoir la longueur unité
# Creations d'une matrice vide avec n lignes et k colonnes
Y <- matrix(0, nrow = n, ncol = k)
# FIci pour chaque élément de X, nous divisons par la racine carré de la somme chaque élément de X au carré
for(i in 1:n){
for(j in 1:k){
Y[i,j] <- X[i,j] / sqrt(sum(X[i,])^2)
}
}
# Normalisons la matrice Y
#Y
## Etape 5: utilisons l'algorithme de K-Means
# traitons chaque ligne de Y comme point de R^k, régroupons les en k régroupements via K-means
km <- kmeans(Y, k, nstart = 2000, algorithm = c("MacQueen"))
return(list(Classe=km$cluster, Inerties=fdm2id::intern(as.numeric(km$cluster), Y, eval = c ("intraclass", "interclass"), type = c("global"))))
# inerties(Y, as.numeric(km$cluster))[-3]
}
#Veiller à choisir le sigma de plus petit I_intra/I_tot.
# Donnees nmist:
data = read_mnist()
#View(data)
images_train=data[["train"]][["images"]][1:1000,] # data$train$images[1:1000,]
save(images_train, file="mnist_reduit")
load("mnist_reduit")
labels_train= data$train$labels[1:1000]
save(labels_train, file="labels_reduit")
load("labels_reduit")
#par(mfrow=c(3,4))
for (k in 1:6)# 12_1ère images
m=matrix(images_train[k,], nrow =28, ncol =28 )
m=t(apply(m, 1, rev))# rev transpose l'odre des éléments d'1 vect.: la dernière col. se retrouve en 1ere position ds la mat.
image(m)
m
##Trouver le meilleur sigma
k=length(unique(labels_train))
sigma =seq(4,13)#spec_clus fct uniquement pour ces valeurs
mm = matrix(NA, nrow = length(sigma), ncol = nrow(images_train))
rap_mnist= vector(mode= "integer", length = length(sigma))
for (j in 1:length(sigma) ){
#mm[j,]=spec_clus(images_train, sigma[j], k)
rap_mnist[j]=spec_clus(images_train, sigma[j], k)$Inerties[1]/(spec_clus(images_train,sigma[j], k)$Inerties[1] +
spec_clus(images_train, sigma[j], k)$Inerties[2])
}
(sigma_opt_mnist=sigma[which.min(rap_mnist)])
table(mm, labels_train)
n <- 100
alpha <- 2
gamma <- 5
eps <- rcauchy(n = n, location = alpha, scale = gamma)
rho <- 0.9
y <- rep(0,n)
for(i in 2:n)
{
y[i] <- rho*y[i-1] + eps[i] 
n <- 100
alpha <- 2
gamma <- 5
eps <- rcauchy(n = n, location = alpha, scale = gamma)
rho <- 0.9
y <- rep(0,n)
for(i in 2:n){
y[i] <- rho*y[i-1] + eps[i] 
n <- 100
alpha <- 2
gamma <- 5
eps <- rcauchy(n = n, location = alpha, scale = gamma)
eps
rho <- 0.9
y <- rep(0,n)
for(i in 2:n){
y[i] <- rho*y[i-1] + eps[i] 
n <- 100
alpha <- 2
gamma <- 5
eps <- rcauchy(n = n, location = alpha, scale = gamma)
eps
rho <- 0.9
y <- rep(0,n)
y[2] <- rho*y[2-1] + eps[2] 
y <- rep(0,n)
y[1]
y[15]
eps[15]
eps[65]
eps[99]
n <- 100
alpha <- 2
gamma <- 5
eps <- rcauchy(n = n, location = alpha, scale = gamma)
eps
rho <- 0.9
y <- rep(0,n)
for(i in 2:n){
y[i] <- rho*y[i-1] + eps[i] 
for(i in 2:n){
y[i] = rho*y[i-1] + eps[i] 
for(i in 2:n){
y[i] = rho*y[i-1] + eps[i] 
for(i in 1:n){
y[i] = rho*y[i] + eps[i+1] 
rho*y[i]
rho*y[1]
EM_gauss <- function(data, nb) {
n <- length(data)
thetaALL <- matrix(0,nb+1,3)
### initialiser theta
theta <- c(runif(1), runif(2, min(data), max(data)))
thetaALL[1,] <- theta
for(k in 1:nb)
{
#calculer p_ij (étape E de calcule de l'espérance, ici, il faut seulement des proba car variable discrète bernoulli pour Z)
p_ij <- matrix(0, n, 2)
for(i in 1:n)
{
p_ij[i,1] <- theta[1]*dnorm(data[i],theta[2])/( theta[1]*dnorm(data[i],theta[2]) + (1-theta[1])*dnorm(data[i],theta[3]))
p_ij[i,2] <- 1 - p_ij[i,1]
}
#update theta (M) trouvé en dévrivant selon chacune des variable recherchées
theta[1] <- mean(p_ij[,1])
theta[2] <- sum(p_ij[,1]*data)/sum(p_ij[,1])
theta[3] <- sum(p_ij[,2]*data)/sum(p_ij[,2])
thetaALL[k+1,] <- theta
}
return(list(theta = theta, thetaAll = thetaALL))
}
##########################
######### le test ########
##########################
EM_gauss(images_train, 30)
##########################
######### le test ########
##########################
EM_gauss(images_train, 4)
EM_gauss(images_train, 5)
EM_gauss(sp$x, 0.7)
EM_gauss(sp$x, 0.4)
EM_gauss(sp$x, 5)
##########################
######### le test ########
##########################
EM_gauss(images_train, 100)
###############################
### EM for gaussian mixture ###
###############################
gaussmixEM = function(params, X, clusters = 2, tol=.00001, maxits=100, showits=T){
# Arguments are starting parameters (means, covariances, cluster probability), data, number of clusters desired, tolerance,
# maximum iterations, and whether to show iterations
# Starting points
N = nrow(X)
nams = names(params)
mu = params$mu
var = params$var
probs = params$probs
# Other initializations
ri = matrix(0, ncol=clusters, nrow=N)  #initialize cluster 'responsibilities', i.e. probability of cluster membership for each observation i
it = 0
converged = FALSE
if (showits)                                  # Show iterations
cat(paste("Iterations of EM:", "\n"))
while ((!converged) & (it < maxits)) {
probsOld = probs
muOld = mu
varOld = var
riOld = ri
### E
# Compute responsibilities
for (k in 1:clusters){
ri[,k] = probs[k] * dnorm(X, mu[k], sd = sqrt(var[k]), log=F)
}
ri = ri/rowSums(ri)
### M
rk = colSums(ri)                           # rk is the weighted average cluster membership size
probs = rk/N
mu = (t(X) %*% ri) / rk                      # could do mu and var via log likelihood here but this is more straightforward
var = (t(X^2) %*% ri) / rk - mu^2
parmlistold = rbind(probsOld, muOld, varOld)
parmlistcurrent = rbind(probs, mu, var)
it = it + 1
if (showits & it == 1 | it%%5 == 0)        # if showits true, & it =1 or divisible by 5 print message
cat(paste(format(it), "...", "\n", sep = ""))
converged = max(abs(parmlistold - parmlistcurrent)) <= tol
}
clust = which(round(ri)==1, arr.ind=T)       # create cluster membership
clust = clust[order(clust[,1]), 2]           # order accoring to row rather than cluster
out = list(probs=probs, mu=mu, var=var, resp=ri, cluster=clust)
}
library(golem); library(shiny); library(testthat)
ui <- fluidPage(
sidebarLayout(
sidebarPanel(
map(names(iris), ~ make_ui(iris[[.x]], .x))
),
mainPanel(
tableOutput("data")
)
)
)
devtools::document()
setwd("~/NewOrdi/API_Recueil/APIname/R")
devtools::document()
devtools::check()
golem::run_dev()
library(tidyverse); library(readr); library(clock); library(lubridate)
death %>% group_by(responsible, season) %>% summarise(nb_deaths_per_responsible=sum(number_of_deaths)) %>% group_by(season)
devtools::document()
devtools::check()
golem::run_dev()
devtools::document()
devtools::check()
golem::run_dev()
golem::run_dev()
library(shiny)
devtools::document()
devtools::check()
golem::run_dev()
devtools::document()
golem::run_dev()
devtools::document()
golem::run_dev()
## Only run examples in interactive R sessions
if (interactive()) {
# demoing group support in the `choices` arg
shinyApp(
ui = fluidPage(
selectInput("state", "Choose a state:",
list(`East Coast` = list("NY", "NJ", "CT"),
`West Coast` = list("WA", "OR", "CA"),
`Midwest` = list("MN", "WI", "IA"))
),
textOutput("result")
),
server = function(input, output) {
output$result <- renderText({
paste("You chose", input$state)
})
}
)
}
## Only run examples in interactive R sessions
if (interactive()) {
# demoing group support in the `choices` arg
shinyApp(
ui = fluidPage(
selectInput("state", "Choose a state:",
list(`East Coast` = list("NY", "NJ", "CT"),
`West Coast` = list("WA", "OR", "CA"),
`Midwest` = list("MN", "WI", "IA"))
),
textOutput("result")
),
server = function(input, output) {
output$result <- renderText({
paste("You chose", input$state)
})
}
)
}
